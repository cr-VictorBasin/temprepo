[centos@data-services-slave cloudflow]$ gcloud container clusters get-credentials global-k8s --region us-east1 --project cr-core-dr-host-project-9eb196
Fetching cluster endpoint and auth data.
kubeconfig entry generated for global-k8s.
[centos@data-services-slave cloudflow]$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories
helm install my-kafka bitnami/kafka --version 25.3.5
[centos@data-services-slave cloudflow]$ helm install my-kafka bitnami/kafka --version 22.1.6
NAME: my-kafka
LAST DEPLOYED: Tue Mar  5 12:18:16 2024
NAMESPACE: cloudflow
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: kafka
CHART VERSION: 25.3.5
APP VERSION: 3.5.1


** Please be patient while the chart is being deployed **

Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:

    my-kafka.cloudflow.svc.cluster.local

Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:

    my-kafka-controller-0.my-kafka-controller-headless.cloudflow.svc.cluster.local:9092
    my-kafka-controller-1.my-kafka-controller-headless.cloudflow.svc.cluster.local:9092
    my-kafka-controller-2.my-kafka-controller-headless.cloudflow.svc.cluster.local:9092

The CLIENT listener for Kafka client connections from within your cluster have been configured with the following security settings:
    - SASL authentication

To connect a client to your Kafka, you need to create the 'client.properties' configuration files with the content below:

security.protocol=SASL_PLAINTEXT
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
    username="user1" \
    password="$(kubectl get secret my-kafka-user-passwords --namespace cloudflow -o jsonpath='{.data.client-passwords}' | base64 -d | cut -d , -f 1)";

To create a pod that you can use as a Kafka client run the following commands:

    kubectl run my-kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.4.1-debian-11-r0 --namespace cloudflow --command -- sleep infinity
    kubectl cp --namespace cloudflow /path/to/client.properties my-kafka-client:/tmp/client.properties
    kubectl exec --tty -i my-kafka-client --namespace cloudflow -- bash

    PRODUCER:
        kafka-console-producer.sh \
            --producer.config /tmp/client.properties \
            --broker-list my-kafka-controller-0.my-kafka-controller-headless.cloudflow.svc.cluster.local:9092,my-kafka-controller-1.my-kafka-controller-headless.cloudflow.svc.cluster.local:9092,my-kafka-controller-2.my-kafka-controller-headless.cloudflow.svc.cluster.local:9092 \
            --topic test

    CONSUMER:
        kafka-console-consumer.sh \
            --consumer.config /tmp/client.properties \
            --bootstrap-server my-kafka.cloudflow.svc.cluster.local:9092 \
            --topic test \
            --from-beginning

WARNING: There are "resources" sections in the chart not set. Using "resourcesPreset" is not recommended for production. For production installations, please set the following values according to your workload needs:
  - controller.resources
+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/



 kubectl run my-kafka-connect --restart='Never' --image docker.io/bitnami/kafka:3.5.1-debian-11-r72 --namespace cloudflow --command -- sleep infinity
 kubectl cp --namespace cloudflow /home/centos/vico/strimzi-kafka-operator/kafka/cloudflow/connect-distributed.properties my-kafka-connect:/tmp/connect-distributed.properties
 kubectl exec --tty -i my-kafka-connect --namespace cloudflow -- bash

     CONNECT:
        connect-distributed.sh \
            /tmp/connect-distributed.properties